//! æ¨¡å‹æœåŠ¡æ¨¡å— - ç°ä»£åŒ–çš„AIæ¨¡å‹æœåŠ¡æ¡†æ¶
//! 
//! æœ¬æ¨¡å—æä¾›å®Œæ•´çš„æ¨¡å‹æœåŠ¡åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
//! - æ¨¡å‹åŠ è½½å’Œç®¡ç†
//! - æ¨ç†APIæœåŠ¡
//! - æ‰¹å¤„ç†æ”¯æŒ
//! - æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶
//! - è´Ÿè½½å‡è¡¡

use crate::{Error, ModelConfig};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use serde::{Deserialize, Serialize};

/// æ¨¡å‹æœåŠ¡çŠ¶æ€
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ModelStatus {
    Loading,
    Ready,
    Serving,
    Error(String),
    Unloading,
}

/// æ¨¡å‹å®ä¾‹
#[derive(Debug, Clone)]
pub struct ModelInstance {
    pub config: ModelConfig,
    pub status: ModelStatus,
    pub load_time: std::time::Instant,
    pub request_count: u64,
    pub error_count: u64,
    pub last_request: Option<std::time::Instant>,
}

/// æ¨ç†è¯·æ±‚
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceRequest {
    pub model_name: String,
    pub inputs: HashMap<String, Vec<f32>>,
    pub parameters: Option<HashMap<String, f32>>,
    pub request_id: Option<String>,
}

/// æ¨ç†å“åº”
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceResponse {
    pub outputs: HashMap<String, Vec<f32>>,
    pub metadata: HashMap<String, String>,
    pub request_id: Option<String>,
    pub processing_time_ms: u64,
}

/// æ‰¹å¤„ç†è¯·æ±‚
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BatchRequest {
    pub requests: Vec<InferenceRequest>,
    pub batch_id: String,
    pub priority: u8, // 1-10, 10ä¸ºæœ€é«˜ä¼˜å…ˆçº§
}

/// æ‰¹å¤„ç†å“åº”
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BatchResponse {
    pub responses: Vec<InferenceResponse>,
    pub batch_id: String,
    pub total_processing_time_ms: u64,
    pub success_count: usize,
    pub error_count: usize,
}

/// æ¨¡å‹æœåŠ¡ç®¡ç†å™¨
#[derive(Debug)]
pub struct ModelServiceManager {
    models: Arc<RwLock<HashMap<String, ModelInstance>>>,
    serving_models: Arc<RwLock<Vec<String>>>, // å½“å‰æ­£åœ¨æœåŠ¡çš„æ¨¡å‹
    request_queue: Arc<RwLock<Vec<InferenceRequest>>>,
    batch_queue: Arc<RwLock<Vec<BatchRequest>>>,
    metrics: Arc<RwLock<HashMap<String, f64>>>,
    max_concurrent_requests: usize,
    batch_size: usize,
    batch_timeout_ms: u64,
}

impl ModelServiceManager {
    /// åˆ›å»ºæ–°çš„æ¨¡å‹æœåŠ¡ç®¡ç†å™¨
    pub fn new() -> Self {
        Self {
            models: Arc::new(RwLock::new(HashMap::new())),
            serving_models: Arc::new(RwLock::new(Vec::new())),
            request_queue: Arc::new(RwLock::new(Vec::new())),
            batch_queue: Arc::new(RwLock::new(Vec::new())),
            metrics: Arc::new(RwLock::new(HashMap::new())),
            max_concurrent_requests: 100,
            batch_size: 32,
            batch_timeout_ms: 100,
        }
    }

    /// ä½¿ç”¨é…ç½®åˆ›å»ºæ¨¡å‹æœåŠ¡ç®¡ç†å™¨
    pub fn with_config(
        max_concurrent_requests: usize,
        batch_size: usize,
        batch_timeout_ms: u64,
    ) -> Self {
        Self {
            models: Arc::new(RwLock::new(HashMap::new())),
            serving_models: Arc::new(RwLock::new(Vec::new())),
            request_queue: Arc::new(RwLock::new(Vec::new())),
            batch_queue: Arc::new(RwLock::new(Vec::new())),
            metrics: Arc::new(RwLock::new(HashMap::new())),
            max_concurrent_requests,
            batch_size,
            batch_timeout_ms,
        }
    }

    /// åŠ è½½æ¨¡å‹
    pub async fn load_model(&self, config: ModelConfig) -> Result<(), Error> {
        tracing::info!("ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹: {}", config.name);
        
        let model_name = config.name.clone();
        let mut models = self.models.write().await;
        
        // æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
        if models.contains_key(&model_name) {
            return Err(Error::ConfigError(format!("æ¨¡å‹ {} å·²ç»åŠ è½½", model_name)));
        }
        
        // åˆ›å»ºæ¨¡å‹å®ä¾‹
        let instance = ModelInstance {
            config: config.clone(),
            status: ModelStatus::Loading,
            load_time: std::time::Instant::now(),
            request_count: 0,
            error_count: 0,
            last_request: None,
        };
        
        models.insert(model_name.clone(), instance);
        
        // æ¨¡æ‹Ÿæ¨¡å‹åŠ è½½è¿‡ç¨‹
        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
        
        // æ›´æ–°æ¨¡å‹çŠ¶æ€ä¸ºå°±ç»ª
        if let Some(instance) = models.get_mut(&model_name) {
            instance.status = ModelStatus::Ready;
            tracing::info!("âœ… æ¨¡å‹ {} åŠ è½½å®Œæˆ", model_name);
        }
        
        // è®°å½•æŒ‡æ ‡
        self.record_metric("models_loaded_total", 1.0).await;
        
        Ok(())
    }

    /// å¸è½½æ¨¡å‹
    pub async fn unload_model(&self, model_name: &str) -> Result<(), Error> {
        tracing::info!("ğŸ—‘ï¸  å¼€å§‹å¸è½½æ¨¡å‹: {}", model_name);
        
        let mut models = self.models.write().await;
        
        if let Some(instance) = models.get_mut(model_name) {
            instance.status = ModelStatus::Unloading;
            
            // æ¨¡æ‹Ÿå¸è½½è¿‡ç¨‹
            tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;
            
            models.remove(model_name);
            tracing::info!("âœ… æ¨¡å‹ {} å¸è½½å®Œæˆ", model_name);
            
            // ä»æœåŠ¡åˆ—è¡¨ä¸­ç§»é™¤
            let mut serving = self.serving_models.write().await;
            serving.retain(|name| name != model_name);
            
            Ok(())
        } else {
            Err(Error::ConfigError(format!("æ¨¡å‹ {} ä¸å­˜åœ¨", model_name)))
        }
    }

    /// å¼€å§‹æœåŠ¡æ¨¡å‹
    pub async fn start_serving(&self, model_name: &str) -> Result<(), Error> {
        let mut models = self.models.write().await;
        
        if let Some(instance) = models.get_mut(model_name) {
            match instance.status {
                ModelStatus::Ready => {
                    instance.status = ModelStatus::Serving;
                    
                    // æ·»åŠ åˆ°æœåŠ¡åˆ—è¡¨
                    let mut serving = self.serving_models.write().await;
                    serving.push(model_name.to_string());
                    
                    tracing::info!("ğŸš€ æ¨¡å‹ {} å¼€å§‹æœåŠ¡", model_name);
                    Ok(())
                }
                ModelStatus::Serving => {
                    tracing::warn!("âš ï¸  æ¨¡å‹ {} å·²ç»åœ¨æœåŠ¡ä¸­", model_name);
                    Ok(())
                }
                _ => Err(Error::ConfigError(format!("æ¨¡å‹ {} çŠ¶æ€ä¸æ­£ç¡®: {:?}", model_name, instance.status))),
            }
        } else {
            Err(Error::ConfigError(format!("æ¨¡å‹ {} ä¸å­˜åœ¨", model_name)))
        }
    }

    /// åœæ­¢æœåŠ¡æ¨¡å‹
    pub async fn stop_serving(&self, model_name: &str) -> Result<(), Error> {
        let mut models = self.models.write().await;
        
        if let Some(instance) = models.get_mut(model_name) {
            instance.status = ModelStatus::Ready;
            
            // ä»æœåŠ¡åˆ—è¡¨ä¸­ç§»é™¤
            let mut serving = self.serving_models.write().await;
            serving.retain(|name| name != model_name);
            
            tracing::info!("â¹ï¸  æ¨¡å‹ {} åœæ­¢æœåŠ¡", model_name);
            Ok(())
        } else {
            Err(Error::ConfigError(format!("æ¨¡å‹ {} ä¸å­˜åœ¨", model_name)))
        }
    }

    /// æ‰§è¡Œæ¨ç†
    pub async fn inference(&self, request: InferenceRequest) -> Result<InferenceResponse, Error> {
        let start_time = std::time::Instant::now();
        
        // æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨æœåŠ¡ä¸­
        {
            let serving = self.serving_models.read().await;
            if !serving.contains(&request.model_name) {
                return Err(Error::ConfigError(format!("æ¨¡å‹ {} æœªåœ¨æœåŠ¡ä¸­", request.model_name)));
            }
        }
        
        // æ›´æ–°æ¨¡å‹ç»Ÿè®¡
        {
            let mut models = self.models.write().await;
            if let Some(instance) = models.get_mut(&request.model_name) {
                instance.request_count += 1;
                instance.last_request = Some(start_time);
            }
        }
        
        // æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹
        let processing_time = match request.model_name.as_str() {
            name if name.contains("transformer") => {
                // Transformeræ¨¡å‹æ¨ç†æ—¶é—´
                tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;
                50
            }
            name if name.contains("cnn") => {
                // CNNæ¨¡å‹æ¨ç†æ—¶é—´
                tokio::time::sleep(tokio::time::Duration::from_millis(30)).await;
                30
            }
            name if name.contains("lstm") => {
                // LSTMæ¨¡å‹æ¨ç†æ—¶é—´
                tokio::time::sleep(tokio::time::Duration::from_millis(40)).await;
                40
            }
            _ => {
                // é»˜è®¤æ¨ç†æ—¶é—´
                tokio::time::sleep(tokio::time::Duration::from_millis(25)).await;
                25
            }
        };
        
        let duration = start_time.elapsed();
        
        // ç”Ÿæˆæ¨¡æ‹Ÿè¾“å‡º
        let mut outputs = HashMap::new();
        for (input_name, input_data) in &request.inputs {
            let output_data = input_data.iter()
                .map(|&x| x * 0.9 + 0.1) // ç®€å•çš„å˜æ¢
                .collect();
            outputs.insert(format!("{}_output", input_name), output_data);
        }
        
        let mut metadata = HashMap::new();
        metadata.insert("model_name".to_string(), request.model_name.clone());
        metadata.insert("processing_time_ms".to_string(), processing_time.to_string());
        metadata.insert("input_count".to_string(), request.inputs.len().to_string());
        
        let response = InferenceResponse {
            outputs,
            metadata,
            request_id: request.request_id,
            processing_time_ms: duration.as_millis() as u64,
        };
        
        // è®°å½•æŒ‡æ ‡
        self.record_metric("inference_requests_total", 1.0).await;
        self.record_metric("inference_processing_time_ms", processing_time as f64).await;
        
        Ok(response)
    }

    /// æ‰¹å¤„ç†æ¨ç†
    pub async fn batch_inference(&self, batch_request: BatchRequest) -> Result<BatchResponse, Error> {
        tracing::info!("ğŸ“¦ å¼€å§‹æ‰¹å¤„ç†æ¨ç†: {} ä¸ªè¯·æ±‚", batch_request.requests.len());
        
        let start_time = std::time::Instant::now();
        let mut responses = Vec::new();
        let mut success_count = 0;
        let mut error_count = 0;
        
        // å¤„ç†è¯·æ±‚ï¼ˆæ‰¹å¤„ç†ä¸­æ‰€æœ‰è¯·æ±‚ä½¿ç”¨ç›¸åŒä¼˜å…ˆçº§ï¼‰
        let sorted_requests = batch_request.requests;
        
        // å¹¶è¡Œå¤„ç†æ‰¹å¤„ç†ä¸­çš„è¯·æ±‚
        let mut tasks = Vec::new();
        
        for request in sorted_requests {
            let manager = self.clone();
            let task = tokio::spawn(async move {
                manager.inference(request).await
            });
            tasks.push(task);
        }
        
        // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for task in tasks {
            match task.await {
                Ok(Ok(response)) => {
                    responses.push(response);
                    success_count += 1;
                }
                Ok(Err(_)) | Err(_) => {
                    error_count += 1;
                }
            }
        }
        
        let total_time = start_time.elapsed();
        
        let response = BatchResponse {
            responses,
            batch_id: batch_request.batch_id,
            total_processing_time_ms: total_time.as_millis() as u64,
            success_count,
            error_count,
        };
        
        // è®°å½•æ‰¹å¤„ç†æŒ‡æ ‡
        self.record_metric("batch_requests_total", 1.0).await;
        self.record_metric("batch_success_rate", success_count as f64 / (success_count + error_count) as f64).await;
        self.record_metric("batch_processing_time_ms", total_time.as_millis() as f64).await;
        
        tracing::info!("âœ… æ‰¹å¤„ç†å®Œæˆ: æˆåŠŸ {} ä¸ªï¼Œå¤±è´¥ {} ä¸ªï¼Œè€—æ—¶ {:.2}ms", 
                      success_count, error_count, total_time.as_millis());
        
        Ok(response)
    }

    /// è·å–æ¨¡å‹çŠ¶æ€
    pub async fn get_model_status(&self, model_name: &str) -> Result<ModelStatus, Error> {
        let models = self.models.read().await;
        
        if let Some(instance) = models.get(model_name) {
            Ok(instance.status.clone())
        } else {
            Err(Error::ConfigError(format!("æ¨¡å‹ {} ä¸å­˜åœ¨", model_name)))
        }
    }

    /// è·å–æ‰€æœ‰æ¨¡å‹çŠ¶æ€
    pub async fn get_all_models(&self) -> HashMap<String, ModelInstance> {
        let models = self.models.read().await;
        models.clone()
    }

    /// è·å–æœåŠ¡ä¸­çš„æ¨¡å‹åˆ—è¡¨
    pub async fn get_serving_models(&self) -> Vec<String> {
        let serving = self.serving_models.read().await;
        serving.clone()
    }

    /// è®°å½•æŒ‡æ ‡
    async fn record_metric(&self, name: &str, value: f64) {
        let mut metrics = self.metrics.write().await;
        metrics.insert(name.to_string(), value);
    }

    /// è·å–æ‰€æœ‰æŒ‡æ ‡
    pub async fn get_metrics(&self) -> HashMap<String, f64> {
        let metrics = self.metrics.read().await;
        metrics.clone()
    }

    /// è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯
    pub async fn get_service_stats(&self) -> HashMap<String, String> {
        let models = self.models.read().await;
        let serving = self.serving_models.read().await;
        let metrics = self.metrics.read().await;
        
        let mut stats = HashMap::new();
        stats.insert("total_models".to_string(), models.len().to_string());
        stats.insert("serving_models".to_string(), serving.len().to_string());
        stats.insert("max_concurrent_requests".to_string(), self.max_concurrent_requests.to_string());
        stats.insert("batch_size".to_string(), self.batch_size.to_string());
        stats.insert("batch_timeout_ms".to_string(), self.batch_timeout_ms.to_string());
        
        // æ·»åŠ æ¨¡å‹ç»Ÿè®¡
        let mut total_requests = 0u64;
        let mut total_errors = 0u64;
        
        for instance in models.values() {
            total_requests += instance.request_count;
            total_errors += instance.error_count;
        }
        
        stats.insert("total_requests".to_string(), total_requests.to_string());
        stats.insert("total_errors".to_string(), total_errors.to_string());
        stats.insert("error_rate".to_string(), 
                    if total_requests > 0 { 
                        format!("{:.2}%", (total_errors as f64 / total_requests as f64) * 100.0)
                    } else { 
                        "0.00%".to_string()
                    });
        
        // æ·»åŠ æ€§èƒ½æŒ‡æ ‡
        for (key, value) in metrics.iter() {
            stats.insert(format!("metric_{}", key), format!("{:.2}", value));
        }
        
        stats
    }
}

impl Clone for ModelServiceManager {
    fn clone(&self) -> Self {
        Self {
            models: Arc::clone(&self.models),
            serving_models: Arc::clone(&self.serving_models),
            request_queue: Arc::clone(&self.request_queue),
            batch_queue: Arc::clone(&self.batch_queue),
            metrics: Arc::clone(&self.metrics),
            max_concurrent_requests: self.max_concurrent_requests,
            batch_size: self.batch_size,
            batch_timeout_ms: self.batch_timeout_ms,
        }
    }
}

impl Default for ModelServiceManager {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::ModelType;

    #[tokio::test]
    async fn test_model_loading() {
        let manager = ModelServiceManager::new();
        
        let config = ModelConfig {
            name: "test_model".to_string(),
            version: "1.0.0".to_string(),
            model_type: ModelType::DeepLearning,
            framework: Some("candle".to_string()),
            parameters: HashMap::new(),
            path: None,
            device: None,
            precision: None,
        };
        
        // åŠ è½½æ¨¡å‹
        assert!(manager.load_model(config).await.is_ok());
        
        // æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        let status = manager.get_model_status("test_model").await.unwrap();
        assert!(matches!(status, ModelStatus::Ready));
        
        // å¸è½½æ¨¡å‹
        assert!(manager.unload_model("test_model").await.is_ok());
    }

    #[tokio::test]
    async fn test_model_serving() {
        let manager = ModelServiceManager::new();
        
        let config = ModelConfig {
            name: "serving_model".to_string(),
            version: "1.0.0".to_string(),
            model_type: ModelType::DeepLearning,
            framework: Some("candle".to_string()),
            parameters: HashMap::new(),
            path: None,
            device: None,
            precision: None,
        };
        
        // åŠ è½½æ¨¡å‹
        manager.load_model(config).await.unwrap();
        
        // å¼€å§‹æœåŠ¡
        assert!(manager.start_serving("serving_model").await.is_ok());
        
        // æ£€æŸ¥æœåŠ¡çŠ¶æ€
        let serving_models = manager.get_serving_models().await;
        assert!(serving_models.contains(&"serving_model".to_string()));
        
        // åœæ­¢æœåŠ¡
        assert!(manager.stop_serving("serving_model").await.is_ok());
    }

    #[tokio::test]
    async fn test_inference() {
        let manager = ModelServiceManager::new();
        
        let config = ModelConfig {
            name: "inference_model".to_string(),
            version: "1.0.0".to_string(),
            model_type: ModelType::DeepLearning,
            framework: Some("candle".to_string()),
            parameters: HashMap::new(),
            path: None,
            device: None,
            precision: None,
        };
        
        // åŠ è½½å¹¶å¼€å§‹æœåŠ¡æ¨¡å‹
        manager.load_model(config).await.unwrap();
        manager.start_serving("inference_model").await.unwrap();
        
        // åˆ›å»ºæ¨ç†è¯·æ±‚
        let mut inputs = HashMap::new();
        inputs.insert("input".to_string(), vec![1.0, 2.0, 3.0]);
        
        let request = InferenceRequest {
            model_name: "inference_model".to_string(),
            inputs,
            parameters: None,
            request_id: Some("test_request".to_string()),
        };
        
        // æ‰§è¡Œæ¨ç†
        let response = manager.inference(request).await.unwrap();
        
        assert_eq!(response.request_id, Some("test_request".to_string()));
        assert!(!response.outputs.is_empty());
        assert!(response.processing_time_ms > 0);
    }

    #[tokio::test]
    async fn test_batch_inference() {
        let manager = ModelServiceManager::new();
        
        let config = ModelConfig {
            name: "batch_model".to_string(),
            version: "1.0.0".to_string(),
            model_type: ModelType::DeepLearning,
            framework: Some("candle".to_string()),
            parameters: HashMap::new(),
            path: None,
            device: None,
            precision: None,
        };
        
        // åŠ è½½å¹¶å¼€å§‹æœåŠ¡æ¨¡å‹
        manager.load_model(config).await.unwrap();
        manager.start_serving("batch_model").await.unwrap();
        
        // åˆ›å»ºæ‰¹å¤„ç†è¯·æ±‚
        let mut requests = Vec::new();
        for i in 0..5 {
            let mut inputs = HashMap::new();
            inputs.insert("input".to_string(), vec![i as f32, (i + 1) as f32]);
            
            let request = InferenceRequest {
                model_name: "batch_model".to_string(),
                inputs,
                parameters: None,
                request_id: Some(format!("batch_request_{}", i)),
            };
            requests.push(request);
        }
        
        let batch_request = BatchRequest {
            requests,
            batch_id: "test_batch".to_string(),
            priority: 5,
        };
        
        // æ‰§è¡Œæ‰¹å¤„ç†æ¨ç†
        let response = manager.batch_inference(batch_request).await.unwrap();
        
        assert_eq!(response.batch_id, "test_batch");
        assert_eq!(response.success_count, 5);
        assert_eq!(response.error_count, 0);
        assert_eq!(response.responses.len(), 5);
    }
}
