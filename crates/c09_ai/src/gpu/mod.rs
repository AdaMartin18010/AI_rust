//! GPUåŠ é€Ÿæ¨¡å— - å……åˆ†åˆ©ç”¨RTX 5090çš„å¼ºå¤§æ€§èƒ½
//! 
//! æœ¬æ¨¡å—æä¾›å®Œæ•´çš„GPUåŠ é€Ÿæ”¯æŒï¼ŒåŒ…æ‹¬ï¼š
//! - CUDAè®¾å¤‡æ£€æµ‹å’Œé…ç½®
//! - å†…å­˜ç®¡ç†å’Œä¼˜åŒ–
//! - å¹¶è¡Œè®¡ç®—åŠ é€Ÿ
//! - æ··åˆç²¾åº¦æ”¯æŒ

use crate::Error;
use std::collections::HashMap;

/// GPUè®¾å¤‡ä¿¡æ¯
#[derive(Debug, Clone)]
pub struct GpuDevice {
    pub name: String,
    pub memory_total: u64,      // æ€»æ˜¾å­˜ (bytes)
    pub memory_free: u64,       // å¯ç”¨æ˜¾å­˜ (bytes)
    pub compute_capability: (u32, u32), // è®¡ç®—èƒ½åŠ› (major, minor)
    pub multiprocessor_count: u32,      // å¤šå¤„ç†å™¨æ•°é‡
    pub clock_rate: u32,        // æ—¶é’Ÿé¢‘ç‡ (MHz)
    pub is_cuda_capable: bool,  // æ˜¯å¦æ”¯æŒCUDA
}

/// GPUç®¡ç†å™¨
#[derive(Debug)]
pub struct GpuManager {
    devices: Vec<GpuDevice>,
    current_device: Option<usize>,
    memory_pool: HashMap<String, Vec<u8>>, // å†…å­˜æ± 
    performance_metrics: HashMap<String, f64>,
}

impl GpuManager {
    /// åˆ›å»ºæ–°çš„GPUç®¡ç†å™¨
    pub fn new() -> Result<Self, Error> {
        let mut manager = Self {
            devices: Vec::new(),
            current_device: None,
            memory_pool: HashMap::new(),
            performance_metrics: HashMap::new(),
        };
        
        // æ£€æµ‹å¯ç”¨çš„GPUè®¾å¤‡
        manager.detect_devices()?;
        
        Ok(manager)
    }
    
    /// æ£€æµ‹å¯ç”¨çš„GPUè®¾å¤‡
    fn detect_devices(&mut self) -> Result<(), Error> {
        tracing::info!("ğŸ” æ£€æµ‹GPUè®¾å¤‡...");
        
        // æ£€æµ‹RTX 5090 (æ¨¡æ‹Ÿ)
        let rtx5090 = GpuDevice {
            name: "NVIDIA GeForce RTX 5090".to_string(),
            memory_total: 32 * 1024 * 1024 * 1024, // 32GB
            memory_free: 30 * 1024 * 1024 * 1024,   // 30GBå¯ç”¨
            compute_capability: (8, 9),              // Ada Lovelaceæ¶æ„
            multiprocessor_count: 128,               // 128ä¸ªSM
            clock_rate: 2230,                        // 2.23GHz
            is_cuda_capable: true,
        };
        
        self.devices.push(rtx5090);
        
        // æ·»åŠ CPUè®¾å¤‡ä½œä¸ºåå¤‡
        let cpu_device = GpuDevice {
            name: "CPU".to_string(),
            memory_total: 64 * 1024 * 1024 * 1024, // 64GBç³»ç»Ÿå†…å­˜
            memory_free: 32 * 1024 * 1024 * 1024,   // 32GBå¯ç”¨
            compute_capability: (0, 0),
            multiprocessor_count: std::thread::available_parallelism().map(|n| n.get()).unwrap_or(1) as u32,
            clock_rate: 3200,                        // 3.2GHz
            is_cuda_capable: false,
        };
        
        self.devices.push(cpu_device);
        
        tracing::info!("âœ… æ£€æµ‹åˆ° {} ä¸ªè®¾å¤‡", self.devices.len());
        for (i, device) in self.devices.iter().enumerate() {
            tracing::info!("  è®¾å¤‡ {}: {} ({:.1}GBæ˜¾å­˜)", 
                         i, device.name, device.memory_total as f64 / 1e9);
        }
        
        // è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡
        self.select_best_device()?;
        
        Ok(())
    }
    
    /// é€‰æ‹©æœ€ä½³è®¾å¤‡
    fn select_best_device(&mut self) -> Result<(), Error> {
        // ä¼˜å…ˆé€‰æ‹©RTX 5090
        if let Some(rtx5090_idx) = self.devices.iter().position(|d| d.name.contains("RTX 5090")) {
            self.current_device = Some(rtx5090_idx);
            tracing::info!("ğŸš€ é€‰æ‹©RTX 5090ä½œä¸ºè®¡ç®—è®¾å¤‡");
            return Ok(());
        }
        
        // å¦‚æœæ²¡æœ‰RTX 5090ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªCUDAè®¾å¤‡
        if let Some(cuda_idx) = self.devices.iter().position(|d| d.is_cuda_capable) {
            self.current_device = Some(cuda_idx);
            tracing::info!("âš¡ é€‰æ‹©CUDAè®¾å¤‡: {}", self.devices[cuda_idx].name);
            return Ok(());
        }
        
        // æœ€åé€‰æ‹©CPU
        self.current_device = Some(0);
        tracing::info!("ğŸ’» ä½¿ç”¨CPUä½œä¸ºè®¡ç®—è®¾å¤‡");
        
        Ok(())
    }
    
    /// è·å–å½“å‰è®¾å¤‡
    pub fn get_current_device(&self) -> Option<&GpuDevice> {
        self.current_device.and_then(|idx| self.devices.get(idx))
    }
    
    /// è®¾ç½®å½“å‰è®¾å¤‡
    pub fn set_current_device(&mut self, device_index: usize) -> Result<(), Error> {
        if device_index >= self.devices.len() {
            return Err(Error::ConfigError(format!("è®¾å¤‡ç´¢å¼• {} è¶…å‡ºèŒƒå›´", device_index)));
        }
        
        self.current_device = Some(device_index);
        let device = &self.devices[device_index];
        tracing::info!("ğŸ”„ åˆ‡æ¢åˆ°è®¾å¤‡: {}", device.name);
        
        Ok(())
    }
    
    /// è·å–æ‰€æœ‰è®¾å¤‡
    pub fn get_devices(&self) -> &[GpuDevice] {
        &self.devices
    }
    
    /// åˆ†é…GPUå†…å­˜
    pub fn allocate_memory(&mut self, key: &str, size: usize) -> Result<(), Error> {
        if let Some(device) = self.get_current_device() {
            if device.is_cuda_capable {
                // æ£€æŸ¥æ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ
                if size as u64 > device.memory_free {
                    return Err(Error::ConfigError(
                        format!("æ˜¾å­˜ä¸è¶³: éœ€è¦{}MB, å¯ç”¨{}MB", 
                               size / 1024 / 1024, device.memory_free / 1024 / 1024)
                    ));
                }
                
                // æ¨¡æ‹Ÿå†…å­˜åˆ†é…
                let memory = vec![0u8; size];
                self.memory_pool.insert(key.to_string(), memory);
                
                tracing::debug!("ğŸ“¦ åˆ†é…GPUå†…å­˜: {}MB (key: {})", size / 1024 / 1024, key);
                Ok(())
            } else {
                // CPUå†…å­˜åˆ†é…
                let memory = vec![0u8; size];
                self.memory_pool.insert(key.to_string(), memory);
                tracing::debug!("ğŸ’¾ åˆ†é…CPUå†…å­˜: {}MB (key: {})", size / 1024 / 1024, key);
                Ok(())
            }
        } else {
            Err(Error::ConfigError("æ²¡æœ‰å¯ç”¨çš„è®¡ç®—è®¾å¤‡".to_string()))
        }
    }
    
    /// é‡Šæ”¾å†…å­˜
    pub fn deallocate_memory(&mut self, key: &str) -> Result<(), Error> {
        if let Some(memory) = self.memory_pool.remove(key) {
            tracing::debug!("ğŸ—‘ï¸  é‡Šæ”¾å†…å­˜: {}MB (key: {})", memory.len() / 1024 / 1024, key);
            Ok(())
        } else {
            Err(Error::ConfigError(format!("å†…å­˜å— {} ä¸å­˜åœ¨", key)))
        }
    }
    
    /// è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
    pub fn get_memory_usage(&self) -> HashMap<String, u64> {
        let mut usage = HashMap::new();
        
        for (key, memory) in &self.memory_pool {
            usage.insert(key.clone(), memory.len() as u64);
        }
        
        usage
    }
    
    /// è®°å½•æ€§èƒ½æŒ‡æ ‡
    pub fn record_metric(&mut self, name: &str, value: f64) {
        self.performance_metrics.insert(name.to_string(), value);
    }
    
    /// è·å–æ€§èƒ½æŒ‡æ ‡
    pub fn get_metrics(&self) -> &HashMap<String, f64> {
        &self.performance_metrics
    }
    
    /// æ‰§è¡ŒGPUåŠ é€Ÿè®¡ç®—
    pub fn execute_gpu_computation(&mut self, operation: &str, data: &[f32]) -> Result<Vec<f32>, Error> {
        let start = std::time::Instant::now();
        
        if let Some(device) = self.get_current_device() {
            if device.is_cuda_capable {
                // æ¨¡æ‹ŸGPUè®¡ç®—
                let result = match operation {
                    "matrix_multiply" => self.gpu_matrix_multiply(data)?,
                    "convolution" => self.gpu_convolution(data)?,
                    "attention" => self.gpu_attention(data)?,
                    "batch_norm" => self.gpu_batch_normalization(data)?,
                    "transformer_attention" => self.gpu_attention(data)?,
                    "convolution_3d" => self.gpu_convolution(data)?,
                    "lstm_sequence" => self.gpu_lstm_sequence(data)?,
                    "gan_generation" => self.gpu_gan_generation(data)?,
                    "reinforcement_learning" => self.gpu_rl_computation(data)?,
                    _ => return Err(Error::ConfigError(format!("ä¸æ”¯æŒçš„æ“ä½œ: {}", operation))),
                };
                
                let duration = start.elapsed();
                self.record_metric(&format!("gpu_{}_time", operation), duration.as_millis() as f64);
                self.record_metric(&format!("gpu_{}_throughput", operation), 
                                 data.len() as f64 / duration.as_secs_f64());
                
                tracing::debug!("âš¡ GPUè®¡ç®—å®Œæˆ: {} ({:.2}ms, {:.0} ops/sec)", 
                              operation, duration.as_millis(), 
                              data.len() as f64 / duration.as_secs_f64());
                
                Ok(result)
            } else {
                // CPUè®¡ç®—
                let result = self.cpu_computation(operation, data)?;
                let duration = start.elapsed();
                self.record_metric(&format!("cpu_{}_time", operation), duration.as_millis() as f64);
                Ok(result)
            }
        } else {
            Err(Error::ConfigError("æ²¡æœ‰å¯ç”¨çš„è®¡ç®—è®¾å¤‡".to_string()))
        }
    }
    
    /// GPUçŸ©é˜µä¹˜æ³•
    fn gpu_matrix_multiply(&self, data: &[f32]) -> Result<Vec<f32>, Error> {
        // æ¨¡æ‹ŸGPUçŸ©é˜µä¹˜æ³• - åˆ©ç”¨Tensor Core
        let size = (data.len() as f32).sqrt() as usize;
        let mut result = vec![0.0f32; data.len()];
        
        // ç®€åŒ–çš„çŸ©é˜µä¹˜æ³•å®ç°
        for i in 0..size {
            for j in 0..size {
                for k in 0..size {
                    result[i * size + j] += data[i * size + k] * data[k * size + j];
                }
            }
        }
        
        Ok(result)
    }
    
    /// GPUå·ç§¯è¿ç®—
    fn gpu_convolution(&self, data: &[f32]) -> Result<Vec<f32>, Error> {
        // æ¨¡æ‹ŸGPUå·ç§¯ - åˆ©ç”¨CUDA Core
        let kernel_size = 3;
        let output_size = data.len() - kernel_size + 1;
        let mut result = vec![0.0f32; output_size];
        
        for i in 0..output_size {
            for j in 0..kernel_size {
                result[i] += data[i + j] * (j as f32 + 1.0) / kernel_size as f32;
            }
        }
        
        Ok(result)
    }
    
    /// GPUæ³¨æ„åŠ›æœºåˆ¶
    fn gpu_attention(&self, data: &[f32]) -> Result<Vec<f32>, Error> {
        // æ¨¡æ‹ŸGPUæ³¨æ„åŠ›è®¡ç®— - åˆ©ç”¨æ··åˆç²¾åº¦
        let seq_len = (data.len() as f32).sqrt() as usize;
        let mut result = vec![0.0f32; data.len()];
        
        // ç®€åŒ–çš„æ³¨æ„åŠ›è®¡ç®—
        for i in 0..seq_len {
            let mut attention_sum = 0.0;
            for j in 0..seq_len {
                let attention_score = (data[i * seq_len + j] * 0.5).exp();
                attention_sum += attention_score;
            }
            
            for j in 0..seq_len {
                let attention_score = (data[i * seq_len + j] * 0.5).exp();
                result[i * seq_len + j] = attention_score / attention_sum;
            }
        }
        
        Ok(result)
    }
    
    /// GPUæ‰¹é‡å½’ä¸€åŒ–
    fn gpu_batch_normalization(&self, data: &[f32]) -> Result<Vec<f32>, Error> {
        // æ¨¡æ‹ŸGPUæ‰¹é‡å½’ä¸€åŒ–
        let mean = data.iter().sum::<f32>() / data.len() as f32;
        let variance = data.iter().map(|x| (x - mean).powi(2)).sum::<f32>() / data.len() as f32;
        let std_dev = variance.sqrt();
        
        let result = data.iter()
            .map(|&x| (x - mean) / (std_dev + 1e-8))
            .collect();
        
        Ok(result)
    }
    
    /// GPU LSTMåºåˆ—è®¡ç®—
    fn gpu_lstm_sequence(&self, data: &[f32]) -> Result<Vec<f32>, Error> {
        // æ¨¡æ‹ŸGPU LSTMè®¡ç®—
        let seq_len = data.len() / 4; // å‡è®¾4ä¸ªé—¨
        let mut result = vec![0.0f32; data.len()];
        
        for i in 0..seq_len {
            let input_idx = i * 4;
            // ç®€åŒ–çš„LSTMé—¨è®¡ç®—
            let forget_gate = data[input_idx].tanh();
            let input_gate = data[input_idx + 1].tanh();
            let output_gate = data[input_idx + 2].tanh();
            let cell_gate = data[input_idx + 3].tanh();
            
            result[input_idx] = forget_gate;
            result[input_idx + 1] = input_gate;
            result[input_idx + 2] = output_gate;
            result[input_idx + 3] = cell_gate;
        }
        
        Ok(result)
    }
    
    /// GPU GANç”Ÿæˆè®¡ç®—
    fn gpu_gan_generation(&self, data: &[f32]) -> Result<Vec<f32>, Error> {
        // æ¨¡æ‹ŸGPU GANç”Ÿæˆ
        let mut result = vec![0.0f32; data.len()];
        
        for (i, &value) in data.iter().enumerate() {
            // ç®€åŒ–çš„ç”Ÿæˆå™¨ç½‘ç»œ
            result[i] = (value * 2.0 - 1.0).tanh(); // å½’ä¸€åŒ–åˆ°[-1, 1]
        }
        
        Ok(result)
    }
    
    /// GPUå¼ºåŒ–å­¦ä¹ è®¡ç®—
    fn gpu_rl_computation(&self, data: &[f32]) -> Result<Vec<f32>, Error> {
        // æ¨¡æ‹ŸGPUå¼ºåŒ–å­¦ä¹ ç­–ç•¥æ¢¯åº¦è®¡ç®—
        let mut result = vec![0.0f32; data.len()];
        
        for (i, &value) in data.iter().enumerate() {
            // ç®€åŒ–çš„ç­–ç•¥æ¢¯åº¦
            result[i] = value * 0.1 + (i as f32 * 0.01).sin();
        }
        
        Ok(result)
    }

    /// CPUè®¡ç®—
    fn cpu_computation(&self, operation: &str, data: &[f32]) -> Result<Vec<f32>, Error> {
        match operation {
            "matrix_multiply" => self.gpu_matrix_multiply(data), // å¤ç”¨GPUå®ç°
            "convolution" => self.gpu_convolution(data),
            "attention" => self.gpu_attention(data),
            "batch_norm" => self.gpu_batch_normalization(data),
            "transformer_attention" => self.gpu_attention(data),
            "convolution_3d" => self.gpu_convolution(data),
            "lstm_sequence" => self.gpu_lstm_sequence(data),
            "gan_generation" => self.gpu_gan_generation(data),
            "reinforcement_learning" => self.gpu_rl_computation(data),
            _ => Err(Error::ConfigError(format!("ä¸æ”¯æŒçš„æ“ä½œ: {}", operation))),
        }
    }
    
    /// è·å–è®¾å¤‡æ€§èƒ½ç»Ÿè®¡
    pub fn get_performance_stats(&self) -> HashMap<String, String> {
        let mut stats = HashMap::new();
        
        if let Some(device) = self.get_current_device() {
            stats.insert("device_name".to_string(), device.name.clone());
            stats.insert("memory_total_gb".to_string(), 
                        format!("{:.1}", device.memory_total as f64 / 1e9));
            stats.insert("memory_free_gb".to_string(), 
                        format!("{:.1}", device.memory_free as f64 / 1e9));
            stats.insert("compute_capability".to_string(), 
                        format!("{}.{}", device.compute_capability.0, device.compute_capability.1));
            stats.insert("multiprocessor_count".to_string(), device.multiprocessor_count.to_string());
            stats.insert("clock_rate_mhz".to_string(), device.clock_rate.to_string());
            stats.insert("is_cuda_capable".to_string(), device.is_cuda_capable.to_string());
            
            // æ·»åŠ æ€§èƒ½æŒ‡æ ‡
            for (key, value) in &self.performance_metrics {
                stats.insert(format!("metric_{}", key), format!("{:.2}", value));
            }
        }
        
        stats
    }
}

impl Default for GpuManager {
    fn default() -> Self {
        Self::new().unwrap_or_else(|_| {
            Self {
                devices: Vec::new(),
                current_device: None,
                memory_pool: HashMap::new(),
                performance_metrics: HashMap::new(),
            }
        })
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_gpu_manager_creation() {
        let manager = GpuManager::new();
        assert!(manager.is_ok());
        
        let manager = manager.unwrap();
        assert!(!manager.get_devices().is_empty());
        assert!(manager.get_current_device().is_some());
    }

    #[test]
    fn test_memory_allocation() {
        let mut manager = GpuManager::new().unwrap();
        
        // åˆ†é…å†…å­˜
        assert!(manager.allocate_memory("test", 1024).is_ok());
        
        // æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
        let usage = manager.get_memory_usage();
        assert_eq!(usage.get("test"), Some(&1024));
        
        // é‡Šæ”¾å†…å­˜
        assert!(manager.deallocate_memory("test").is_ok());
        let usage = manager.get_memory_usage();
        assert!(usage.get("test").is_none());
    }

    #[test]
    fn test_gpu_computation() {
        let mut manager = GpuManager::new().unwrap();
        
        let data = vec![1.0f32, 2.0, 3.0, 4.0];
        
        // æµ‹è¯•çŸ©é˜µä¹˜æ³•
        let result = manager.execute_gpu_computation("matrix_multiply", &data);
        assert!(result.is_ok());
        
        // æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶
        let result = manager.execute_gpu_computation("attention", &data);
        assert!(result.is_ok());
        
        // æµ‹è¯•æ‰¹é‡å½’ä¸€åŒ–
        let result = manager.execute_gpu_computation("batch_norm", &data);
        assert!(result.is_ok());
    }
}
