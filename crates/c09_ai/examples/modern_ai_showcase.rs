//! ç°ä»£åŒ–AIç³»ç»Ÿå±•ç¤º - å……åˆ†åˆ©ç”¨RTX 5090æ€§èƒ½
//! 
//! æœ¬ç¤ºä¾‹å±•ç¤ºäº†AI-Rusté¡¹ç›®çš„ç°ä»£åŒ–AIç³»ç»ŸåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
//! - é«˜æ€§èƒ½GPUåŠ é€Ÿæ¨ç†
//! - ç°ä»£äº‹ä»¶é©±åŠ¨æ¶æ„
//! - å®æ—¶æ€§èƒ½ç›‘æ§
//! - å¤šæ¨¡æ€AIå¤„ç†
//! - åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦

use c19_ai::*;
use std::time::{Duration, Instant};
use tokio::time::sleep;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    tracing_subscriber::fmt::init();
    
    println!("ğŸš€ AI-Rust ç°ä»£åŒ–AIç³»ç»Ÿå±•ç¤º");
    println!("ğŸ¯ ç›®æ ‡ï¼šå……åˆ†åˆ©ç”¨RTX 5090çš„å¼ºå¤§æ€§èƒ½");
    println!("{}", "=".repeat(60));

    // åˆ›å»ºé«˜æ€§èƒ½AIå¼•æ“
    let mut engine = create_high_performance_engine().await?;
    
    // å±•ç¤ºç°ä»£åŒ–AIåŠŸèƒ½
    demonstrate_modern_ai_features(&mut engine).await?;
    
    // å±•ç¤ºGPUåŠ é€Ÿæ€§èƒ½
    demonstrate_gpu_acceleration(&mut engine).await?;
    
    // å±•ç¤ºäº‹ä»¶é©±åŠ¨æ¶æ„
    demonstrate_event_driven_architecture(&mut engine).await?;
    
    // å±•ç¤ºå®æ—¶ç›‘æ§
    demonstrate_real_time_monitoring(&mut engine).await?;
    
    // å±•ç¤ºå¤šæ¨¡æ€AIå¤„ç†
    demonstrate_multimodal_ai(&mut engine).await?;
    
    // å±•ç¤ºåˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦
    demonstrate_distributed_scheduling(&mut engine).await?;
    
    // æ€§èƒ½åŸºå‡†æµ‹è¯•
    run_performance_benchmarks(&mut engine).await?;
    
    println!("\nğŸ‰ ç°ä»£åŒ–AIç³»ç»Ÿå±•ç¤ºå®Œæˆï¼");
    println!("ğŸ“Š ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯ï¼š");
    print_system_stats(&engine);
    
    // æ¸…ç†èµ„æº
    engine.cleanup()?;
    println!("âœ… èµ„æºæ¸…ç†å®Œæˆ");
    
    Ok(())
}

/// åˆ›å»ºé«˜æ€§èƒ½AIå¼•æ“
async fn create_high_performance_engine() -> Result<AIEngine, Error> {
    println!("\nğŸ”§ åˆ›å»ºé«˜æ€§èƒ½AIå¼•æ“...");
    
    let mut config = EngineConfig::default();
    config.enable_gpu = true;           // å¯ç”¨GPUåŠ é€Ÿ
    config.max_models = 50;             // æ”¯æŒæ›´å¤šæ¨¡å‹
    config.cache_size = 10000;          // æ›´å¤§çš„ç¼“å­˜
    config.enable_monitoring = true;    // å¯ç”¨ç›‘æ§
    config.mixed_precision = true;      // æ··åˆç²¾åº¦åŠ é€Ÿ
    
    let mut engine = AIEngine::with_config(config);
    
    // è®¾ç½®GPUè®¾å¤‡ï¼ˆRTX 5090ï¼‰
    if let Err(_) = engine.set_device("cuda".to_string()) {
        println!("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼");
        engine.set_device("cpu".to_string())?;
    } else {
        println!("âœ… RTX 5090 GPUåŠ é€Ÿå·²å¯ç”¨");
    }
    
    // æ³¨å†Œç°ä»£åŒ–AIæ¨¡å—
    register_modern_ai_modules(&mut engine);
    
    // è®¾ç½®èµ„æºé™åˆ¶
    engine.set_resource_limit("max_concurrent_tasks", 100)?;
    engine.set_resource_limit("memory_limit_gb", 32)?;  // RTX 5090çš„32GBæ˜¾å­˜
    
    println!("âœ… é«˜æ€§èƒ½AIå¼•æ“åˆ›å»ºå®Œæˆ");
    Ok(engine)
}

/// æ³¨å†Œç°ä»£åŒ–AIæ¨¡å—
fn register_modern_ai_modules(engine: &mut AIEngine) {
    println!("ğŸ“¦ æ³¨å†Œç°ä»£åŒ–AIæ¨¡å—...");
    
    // å¤§è¯­è¨€æ¨¡å‹æ¨¡å—
    let mut llm_module = AIModule::new(
        "å¤§è¯­è¨€æ¨¡å‹".to_string(),
        "æ”¯æŒGPTã€LLaMAã€Claudeç­‰ç°ä»£å¤§è¯­è¨€æ¨¡å‹".to_string()
    );
    llm_module.add_capability("æ–‡æœ¬ç”Ÿæˆ".to_string());
    llm_module.add_capability("å¯¹è¯ç³»ç»Ÿ".to_string());
    llm_module.add_capability("ä»£ç ç”Ÿæˆ".to_string());
    llm_module.add_capability("å¤šè¯­è¨€æ”¯æŒ".to_string());
    llm_module.set_framework("candle".to_string());
    llm_module.add_device("cuda".to_string());
    engine.register_module(llm_module);
    
    // è®¡ç®—æœºè§†è§‰æ¨¡å—
    let mut cv_module = AIModule::new(
        "è®¡ç®—æœºè§†è§‰".to_string(),
        "æ”¯æŒå›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒç”Ÿæˆç­‰CVä»»åŠ¡".to_string()
    );
    cv_module.add_capability("å›¾åƒåˆ†ç±»".to_string());
    cv_module.add_capability("ç›®æ ‡æ£€æµ‹".to_string());
    cv_module.add_capability("å›¾åƒåˆ†å‰²".to_string());
    cv_module.add_capability("å›¾åƒç”Ÿæˆ".to_string());
    cv_module.add_capability("è§†é¢‘åˆ†æ".to_string());
    cv_module.set_framework("candle".to_string());
    cv_module.add_device("cuda".to_string());
    engine.register_module(cv_module);
    
    // å¤šæ¨¡æ€AIæ¨¡å—
    let mut multimodal_module = AIModule::new(
        "å¤šæ¨¡æ€AI".to_string(),
        "æ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§æ¨¡æ€çš„AIå¤„ç†".to_string()
    );
    multimodal_module.add_capability("å›¾æ–‡ç†è§£".to_string());
    multimodal_module.add_capability("è§†è§‰é—®ç­”".to_string());
    multimodal_module.add_capability("å›¾åƒæè¿°ç”Ÿæˆ".to_string());
    multimodal_module.add_capability("å¤šæ¨¡æ€æ£€ç´¢".to_string());
    multimodal_module.set_framework("candle".to_string());
    multimodal_module.add_device("cuda".to_string());
    engine.register_module(multimodal_module);
    
    // å¼ºåŒ–å­¦ä¹ æ¨¡å—
    let mut rl_module = AIModule::new(
        "å¼ºåŒ–å­¦ä¹ ".to_string(),
        "æ”¯æŒæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œæ™ºèƒ½ä½“è®­ç»ƒ".to_string()
    );
    rl_module.add_capability("ç­–ç•¥æ¢¯åº¦".to_string());
    rl_module.add_capability("Qå­¦ä¹ ".to_string());
    rl_module.add_capability("Actor-Critic".to_string());
    rl_module.add_capability("å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ".to_string());
    rl_module.set_framework("candle".to_string());
    rl_module.add_device("cuda".to_string());
    engine.register_module(rl_module);
    
    println!("âœ… å·²æ³¨å†Œ {} ä¸ªç°ä»£åŒ–AIæ¨¡å—", engine.get_modules().len());
}

/// å±•ç¤ºç°ä»£åŒ–AIåŠŸèƒ½
async fn demonstrate_modern_ai_features(engine: &mut AIEngine) -> Result<(), Error> {
    println!("\nğŸ¤– å±•ç¤ºç°ä»£åŒ–AIåŠŸèƒ½...");
    
    // è®¾ç½®ç³»ç»ŸçŠ¶æ€
    engine.set_state("ai_mode", "production")?;
    engine.set_state("performance_level", "high")?;
    engine.set_state("gpu_utilization", "85%")?;
    
    // è®°å½•æ€§èƒ½æŒ‡æ ‡
    engine.record_metric("inference_speed", 1250.0);  // tokens/second
    engine.record_metric("gpu_memory_usage", 24.5);   // GB
    engine.record_metric("model_accuracy", 0.95);
    engine.record_metric("latency_p99", 50.0);        // ms
    
    println!("âœ… ç³»ç»ŸçŠ¶æ€å’ŒæŒ‡æ ‡å·²è®¾ç½®");
    
    // æ¨¡æ‹ŸAIæ¨ç†ä»»åŠ¡
    for i in 0..5 {
        let start = Instant::now();
        
        // æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹
        let result = engine.predict(&format!("åˆ†æè¿™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼šè¿™æ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„AIç³»ç»Ÿ")).await?;
        
        let duration = start.elapsed();
        engine.record_metric(&format!("inference_time_{}", i), duration.as_millis() as f64);
        
        println!("ğŸ“Š æ¨ç†ä»»åŠ¡ {}: {:.2}ms, ç½®ä¿¡åº¦: {:.2}", 
                i + 1, duration.as_millis(), result.confidence);
        
        sleep(Duration::from_millis(10)).await;
    }
    
    Ok(())
}

/// å±•ç¤ºGPUåŠ é€Ÿæ€§èƒ½
async fn demonstrate_gpu_acceleration(engine: &mut AIEngine) -> Result<(), Error> {
    println!("\nâš¡ å±•ç¤ºGPUåŠ é€Ÿæ€§èƒ½...");
    
    // è®¾ç½®GPUç›¸å…³çŠ¶æ€
    engine.set_state("gpu_device", "RTX 5090")?;
    engine.set_state("gpu_memory", "32GB")?;
    engine.set_state("compute_capability", "8.9")?;
    
    // æ¨¡æ‹Ÿå¤§æ‰¹é‡å¤„ç†
    let batch_sizes = vec![1, 10, 100, 1000];
    
    for batch_size in batch_sizes {
        let start = Instant::now();
        
        // æ¨¡æ‹Ÿæ‰¹é‡æ¨ç†
        for _ in 0..batch_size {
            engine.predict("GPUåŠ é€Ÿæ¨ç†æµ‹è¯•").await?;
        }
        
        let duration = start.elapsed();
        let throughput = batch_size as f64 / duration.as_secs_f64();
        
        engine.record_metric(&format!("gpu_throughput_batch_{}", batch_size), throughput);
        
        println!("ğŸš€ æ‰¹é‡å¤§å° {}: {:.2} samples/sec, è€—æ—¶: {:.2}ms", 
                batch_size, throughput, duration.as_millis());
    }
    
    Ok(())
}

/// å±•ç¤ºäº‹ä»¶é©±åŠ¨æ¶æ„
async fn demonstrate_event_driven_architecture(engine: &mut AIEngine) -> Result<(), Error> {
    println!("\nğŸ¯ å±•ç¤ºäº‹ä»¶é©±åŠ¨æ¶æ„...");
    
    // æ³¨å†Œäº‹ä»¶ç›‘å¬å™¨
    let task_count = std::sync::Arc::new(std::sync::atomic::AtomicUsize::new(0));
    let task_count_clone = task_count.clone();
    
    engine.on_event("task_completed", move |_data| {
        task_count_clone.fetch_add(1, std::sync::atomic::Ordering::SeqCst);
        println!("ğŸ“‹ ä»»åŠ¡å®Œæˆäº‹ä»¶è§¦å‘");
    })?;
    
    let error_count = std::sync::Arc::new(std::sync::atomic::AtomicUsize::new(0));
    let error_count_clone = error_count.clone();
    
    engine.on_event("error_occurred", move |data| {
        error_count_clone.fetch_add(1, std::sync::atomic::Ordering::SeqCst);
        println!("âŒ é”™è¯¯äº‹ä»¶: {}", data);
    })?;
    
    // è§¦å‘å„ç§äº‹ä»¶
    for i in 0..10 {
        engine.emit_event("task_completed", &format!("task_{}", i))?;
        
        if i % 3 == 0 {
            engine.emit_event("error_occurred", &format!("æ¨¡æ‹Ÿé”™è¯¯_{}", i))?;
        }
        
        sleep(Duration::from_millis(5)).await;
    }
    
    println!("âœ… äº‹ä»¶ç»Ÿè®¡ - å®Œæˆä»»åŠ¡: {}, é”™è¯¯äº‹ä»¶: {}", 
            task_count.load(std::sync::atomic::Ordering::SeqCst),
            error_count.load(std::sync::atomic::Ordering::SeqCst));
    
    Ok(())
}

/// å±•ç¤ºå®æ—¶ç›‘æ§
async fn demonstrate_real_time_monitoring(engine: &mut AIEngine) -> Result<(), Error> {
    println!("\nğŸ“Š å±•ç¤ºå®æ—¶ç›‘æ§...");
    
    // æ¨¡æ‹Ÿå®æ—¶ç›‘æ§æ•°æ®
    for i in 0..20 {
        // æ›´æ–°ç³»ç»ŸæŒ‡æ ‡
        engine.record_metric("cpu_usage", 45.0 + (i as f64 * 0.5));
        engine.record_metric("gpu_usage", 60.0 + (i as f64 * 1.0));
        engine.record_metric("memory_usage", 8.5 + (i as f64 * 0.1));
        engine.record_metric("queue_length", (i % 10) as f64);
        
        // æ¯5æ¬¡æ›´æ–°æ‰“å°ä¸€æ¬¡çŠ¶æ€
        if i % 5 == 0 {
            let metrics = engine.get_metrics();
            println!("ğŸ“ˆ å®æ—¶æŒ‡æ ‡ - GPUä½¿ç”¨ç‡: {:.1}%, å†…å­˜: {:.1}GB, é˜Ÿåˆ—é•¿åº¦: {:.0}", 
                    metrics.get("gpu_usage").unwrap_or(&0.0),
                    metrics.get("memory_usage").unwrap_or(&0.0),
                    metrics.get("queue_length").unwrap_or(&0.0));
        }
        
        sleep(Duration::from_millis(50)).await;
    }
    
    Ok(())
}

/// å±•ç¤ºå¤šæ¨¡æ€AIå¤„ç†
async fn demonstrate_multimodal_ai(engine: &mut AIEngine) -> Result<(), Error> {
    println!("\nğŸ¨ å±•ç¤ºå¤šæ¨¡æ€AIå¤„ç†...");
    
    // è®¾ç½®å¤šæ¨¡æ€å¤„ç†çŠ¶æ€
    engine.set_state("multimodal_mode", "enabled")?;
    engine.set_state("supported_modalities", "text,image,audio,video")?;
    
    // æ¨¡æ‹Ÿå¤šæ¨¡æ€ä»»åŠ¡
    let modalities = vec![
        ("æ–‡æœ¬åˆ†æ", "åˆ†æç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬å†…å®¹"),
        ("å›¾åƒè¯†åˆ«", "è¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“å’Œåœºæ™¯"),
        ("éŸ³é¢‘å¤„ç†", "å¤„ç†è¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘åˆ†æ"),
        ("è§†é¢‘ç†è§£", "åˆ†æè§†é¢‘å†…å®¹å’ŒåŠ¨ä½œ"),
        ("å›¾æ–‡ç†è§£", "ç†è§£å›¾åƒå’Œæ–‡æœ¬çš„å…³è”"),
    ];
    
    for (modality, description) in modalities {
        let start = Instant::now();
        
        // æ¨¡æ‹Ÿå¤šæ¨¡æ€å¤„ç†
        engine.predict(&format!("å¤šæ¨¡æ€å¤„ç†: {}", description)).await?;
        
        let duration = start.elapsed();
        engine.record_metric(&format!("{}_processing_time", modality), duration.as_millis() as f64);
        
        println!("ğŸ¯ {}: {:.2}ms", modality, duration.as_millis());
        
        sleep(Duration::from_millis(20)).await;
    }
    
    Ok(())
}

/// å±•ç¤ºåˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦
async fn demonstrate_distributed_scheduling(engine: &mut AIEngine) -> Result<(), Error> {
    println!("\nğŸ”„ å±•ç¤ºåˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦...");
    
    // æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—
    let tasks = vec![
        "åˆ†å¸ƒå¼è®­ç»ƒä»»åŠ¡_1".to_string(),
        "æ¨¡å‹æ¨ç†ä»»åŠ¡_2".to_string(),
        "æ•°æ®é¢„å¤„ç†ä»»åŠ¡_3".to_string(),
        "æ¨¡å‹è¯„ä¼°ä»»åŠ¡_4".to_string(),
        "ç»“æœèšåˆä»»åŠ¡_5".to_string(),
    ];
    
    for task in tasks {
        engine.add_task(task)?;
    }
    
    println!("ğŸ“‹ ä»»åŠ¡é˜Ÿåˆ—é•¿åº¦: {}", engine.get_task_queue_length());
    
    // æ¨¡æ‹Ÿä»»åŠ¡è°ƒåº¦
    let mut completed_tasks = 0;
    while let Some(task) = engine.get_next_task() {
        println!("âš¡ æ‰§è¡Œä»»åŠ¡: {}", task);
        
        // æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œ
        sleep(Duration::from_millis(30)).await;
        
        // è§¦å‘ä»»åŠ¡å®Œæˆäº‹ä»¶
        engine.emit_event("task_completed", &task)?;
        
        completed_tasks += 1;
        
        if completed_tasks >= 3 {
            break; // åªæ‰§è¡Œå‰3ä¸ªä»»åŠ¡ä½œä¸ºæ¼”ç¤º
        }
    }
    
    println!("âœ… å·²å®Œæˆ {} ä¸ªä»»åŠ¡ï¼Œå‰©ä½™é˜Ÿåˆ—é•¿åº¦: {}", 
            completed_tasks, engine.get_task_queue_length());
    
    Ok(())
}

/// è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
async fn run_performance_benchmarks(engine: &mut AIEngine) -> Result<(), Error> {
    println!("\nğŸƒ è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...");
    
    let benchmarks = vec![
        ("å•æ¬¡æ¨ç†å»¶è¿Ÿ", 1),
        ("å°æ‰¹é‡å¤„ç†", 10),
        ("ä¸­æ‰¹é‡å¤„ç†", 100),
        ("å¤§æ‰¹é‡å¤„ç†", 1000),
    ];
    
    for (name, iterations) in benchmarks {
        let start = Instant::now();
        
        for _ in 0..iterations {
            engine.predict("æ€§èƒ½åŸºå‡†æµ‹è¯•").await?;
        }
        
        let duration = start.elapsed();
        let throughput = iterations as f64 / duration.as_secs_f64();
        let avg_latency = duration.as_millis() as f64 / iterations as f64;
        
        engine.record_metric(&format!("{}_throughput", name.replace(" ", "_")), throughput);
        engine.record_metric(&format!("{}_latency", name.replace(" ", "_")), avg_latency);
        
        println!("ğŸ“Š {}: {:.2} ops/sec, å¹³å‡å»¶è¿Ÿ: {:.2}ms", 
                name, throughput, avg_latency);
    }
    
    Ok(())
}

/// æ‰“å°ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
fn print_system_stats(engine: &AIEngine) {
    let stats = engine.get_stats();
    
    println!("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("â”‚                   ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯                           â”‚");
    println!("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
    println!("â”‚ å¼•æ“ç‰ˆæœ¬: {:<45} â”‚", stats.get("version").unwrap_or(&"æœªçŸ¥".to_string()));
    println!("â”‚ è¿è¡Œæ—¶é—´: {:<45} â”‚", format!("{}ç§’", stats.get("uptime_seconds").unwrap_or(&"0".to_string())));
    println!("â”‚ æ¨¡å—æ•°é‡: {:<45} â”‚", stats.get("modules_count").unwrap_or(&"0".to_string()));
    println!("â”‚ æ¨¡å‹æ•°é‡: {:<45} â”‚", stats.get("models_count").unwrap_or(&"0".to_string()));
    println!("â”‚ çŠ¶æ€æ¡ç›®: {:<45} â”‚", stats.get("state_entries").unwrap_or(&"0".to_string()));
    println!("â”‚ æŒ‡æ ‡æ•°é‡: {:<45} â”‚", stats.get("metrics_count").unwrap_or(&"0".to_string()));
    println!("â”‚ ç¼“å­˜å¤§å°: {:<45} â”‚", stats.get("cache_size").unwrap_or(&"0".to_string()));
    println!("â”‚ é˜Ÿåˆ—é•¿åº¦: {:<45} â”‚", stats.get("task_queue_length").unwrap_or(&"0".to_string()));
    println!("â”‚ è¿è¡ŒçŠ¶æ€: {:<45} â”‚", stats.get("is_running").unwrap_or(&"false".to_string()));
    println!("â”‚ å½“å‰è®¾å¤‡: {:<45} â”‚", stats.get("current_device").unwrap_or(&"æœªçŸ¥".to_string()));
    println!("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
    
    // æ˜¾ç¤ºå…³é”®æ€§èƒ½æŒ‡æ ‡
    let metrics = engine.get_metrics();
    println!("\nğŸš€ å…³é”®æ€§èƒ½æŒ‡æ ‡:");
    println!("   â€¢ GPUä½¿ç”¨ç‡: {:.1}%", metrics.get("gpu_usage").unwrap_or(&0.0));
    println!("   â€¢ æ¨ç†ååé‡: {:.0} ops/sec", metrics.get("å¤§æ‰¹é‡å¤„ç†_throughput").unwrap_or(&0.0));
    println!("   â€¢ å¹³å‡å»¶è¿Ÿ: {:.2} ms", metrics.get("å•æ¬¡æ¨ç†å»¶è¿Ÿ_latency").unwrap_or(&0.0));
    println!("   â€¢ å†…å­˜ä½¿ç”¨: {:.1} GB", metrics.get("memory_usage").unwrap_or(&0.0));
}
