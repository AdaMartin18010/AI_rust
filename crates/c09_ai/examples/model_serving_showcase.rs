//! æ¨¡å‹æœåŠ¡å±•ç¤º - ç°ä»£åŒ–çš„AIæ¨¡å‹æœåŠ¡æ¡†æ¶
//! 
//! æœ¬ç¤ºä¾‹å±•ç¤ºäº†AI-Rusté¡¹ç›®çš„æ¨¡å‹æœåŠ¡åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
//! - æ¨¡å‹åŠ è½½å’Œç®¡ç†
//! - æ¨ç†APIæœåŠ¡
//! - æ‰¹å¤„ç†æ”¯æŒ
//! - è´Ÿè½½å‡è¡¡
//! - æ€§èƒ½ç›‘æ§

use c19_ai::*;
use std::time::{Duration, Instant};
use std::collections::HashMap;
use tokio::time::sleep;
use serde_json::Value;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    tracing_subscriber::fmt::init();
    
    println!("ğŸš€ AI-Rust æ¨¡å‹æœåŠ¡å±•ç¤º");
    println!("ğŸ¯ ç›®æ ‡ï¼šå±•ç¤ºç°ä»£åŒ–çš„AIæ¨¡å‹æœåŠ¡æ¡†æ¶");
    println!("{}", "=".repeat(60));

    // åˆ›å»ºæ¨¡å‹æœåŠ¡å¼•æ“
    let mut engine = create_model_service_engine().await?;
    
    // å±•ç¤ºæ¨¡å‹åŠ è½½å’Œç®¡ç†
    demonstrate_model_loading(&mut engine).await?;
    
    // å±•ç¤ºæ¨ç†APIæœåŠ¡
    demonstrate_inference_api(&mut engine).await?;
    
    // å±•ç¤ºæ‰¹å¤„ç†æœåŠ¡
    demonstrate_batch_processing(&mut engine).await?;
    
    // å±•ç¤ºè´Ÿè½½å‡è¡¡
    demonstrate_load_balancing(&mut engine).await?;
    
    // å±•ç¤ºæ€§èƒ½ç›‘æ§
    demonstrate_performance_monitoring(&mut engine).await?;
    
    // å±•ç¤ºæ¨¡å‹ç‰ˆæœ¬ç®¡ç†
    demonstrate_model_versioning(&mut engine).await?;
    
    println!("\nğŸ‰ æ¨¡å‹æœåŠ¡å±•ç¤ºå®Œæˆï¼");
    println!("ğŸ“Š æ¨¡å‹æœåŠ¡ç»Ÿè®¡ä¿¡æ¯ï¼š");
    print_model_service_stats(&engine).await;
    
    // æ¸…ç†èµ„æº
    engine.cleanup()?;
    println!("âœ… èµ„æºæ¸…ç†å®Œæˆ");
    
    Ok(())
}

/// åˆ›å»ºæ¨¡å‹æœåŠ¡å¼•æ“
async fn create_model_service_engine() -> Result<AIEngine, Error> {
    println!("\nğŸ”§ åˆ›å»ºæ¨¡å‹æœåŠ¡å¼•æ“...");
    
    let mut config = EngineConfig::default();
    config.enable_gpu = true;
    config.max_models = 50;             // æ”¯æŒ50ä¸ªæ¨¡å‹
    config.cache_size = 20000;          // å¤§ç¼“å­˜
    config.enable_monitoring = true;
    config.mixed_precision = true;
    
    let mut engine = AIEngine::with_config(config);
    
    // è®¾ç½®æ¨¡å‹æœåŠ¡çŠ¶æ€
    engine.set_state("model_service_mode", "production")?;
    engine.set_state("max_concurrent_requests", "100")?;
    engine.set_state("batch_size", "32")?;
    engine.set_state("load_balancing", "enabled")?;
    
    println!("âœ… æ¨¡å‹æœåŠ¡å¼•æ“åˆ›å»ºå®Œæˆ");
    Ok(engine)
}

/// å±•ç¤ºæ¨¡å‹åŠ è½½å’Œç®¡ç†
async fn demonstrate_model_loading(engine: &mut AIEngine) -> Result<(), Error> {
    println!("\nğŸ“¦ å±•ç¤ºæ¨¡å‹åŠ è½½å’Œç®¡ç†...");
    
    // åˆ›å»ºä¸åŒç±»å‹çš„æ¨¡å‹
    let models = vec![
        create_model_config("transformer_bert", "BERTè¯­è¨€æ¨¡å‹", ModelType::NLP),
        create_model_config("cnn_resnet50", "ResNet50å›¾åƒåˆ†ç±»", ModelType::ComputerVision),
        create_model_config("lstm_sentiment", "LSTMæƒ…æ„Ÿåˆ†æ", ModelType::NLP),
        create_model_config("transformer_gpt", "GPTæ–‡æœ¬ç”Ÿæˆ", ModelType::NLP),
        create_model_config("cnn_yolo", "YOLOç›®æ ‡æ£€æµ‹", ModelType::ComputerVision),
    ];
    
    // åŠ è½½æ¨¡å‹
    for model in models {
        println!("ğŸ”„ åŠ è½½æ¨¡å‹: {} ({})", model.name, model.version);
        
        let start = Instant::now();
        engine.load_model_to_service(model.clone()).await?;
        let load_time = start.elapsed();
        
        // å¼€å§‹æœåŠ¡æ¨¡å‹
        engine.start_model_serving(&model.name).await?;
        
        println!("   âœ… åŠ è½½å®Œæˆï¼Œè€—æ—¶: {:.2}ms", load_time.as_millis());
        println!("   ğŸš€ å¼€å§‹æœåŠ¡æ¨¡å‹");
        
        sleep(Duration::from_millis(50)).await;
    }
    
    // æ˜¾ç¤ºå·²åŠ è½½çš„æ¨¡å‹
    let all_models = engine.get_model_service().get_all_models().await;
    println!("ğŸ“Š å·²åŠ è½½æ¨¡å‹æ•°é‡: {}", all_models.len());
    
    for (name, instance) in &all_models {
        println!("   â€¢ {}: {:?}", name, instance.status);
    }
    
    Ok(())
}

/// å±•ç¤ºæ¨ç†APIæœåŠ¡
async fn demonstrate_inference_api(engine: &mut AIEngine) -> Result<(), Error> {
    println!("\nğŸ”® å±•ç¤ºæ¨ç†APIæœåŠ¡...");
    
    // åˆ›å»ºä¸åŒç±»å‹çš„æ¨ç†è¯·æ±‚
    let inference_scenarios = vec![
        ("transformer_bert", "æ–‡æœ¬åˆ†ç±»", vec![1.0, 2.0, 3.0, 4.0, 5.0]),
        ("cnn_resnet50", "å›¾åƒåˆ†ç±»", vec![0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]),
        ("lstm_sentiment", "æƒ…æ„Ÿåˆ†æ", vec![0.5, 0.3, 0.8, 0.2, 0.9]),
        ("transformer_gpt", "æ–‡æœ¬ç”Ÿæˆ", vec![0.1, 0.9, 0.3, 0.7, 0.2]),
        ("cnn_yolo", "ç›®æ ‡æ£€æµ‹", vec![0.1; 100]),
    ];
    
    for (model_name, task_type, input_data) in inference_scenarios {
        println!("ğŸ¯ æ‰§è¡Œæ¨ç†: {} - {}", model_name, task_type);
        
        let mut inputs = HashMap::new();
        inputs.insert("input".to_string(), input_data);
        
        let request = model_serving::InferenceRequest {
            model_name: model_name.to_string(),
            inputs,
            parameters: None,
            request_id: Some(format!("req_{}", Instant::now().elapsed().as_millis())),
        };
        
        let start = Instant::now();
        let response = engine.inference(request).await?;
        let duration = start.elapsed();
        
        println!("   ğŸ“Š è¾“å‡ºæ•°é‡: {}", response.outputs.len());
        println!("   â±ï¸  æ¨ç†æ—¶é—´: {:.2}ms", duration.as_millis());
        println!("   ğŸ†” è¯·æ±‚ID: {:?}", response.request_id);
        
        // æ˜¾ç¤ºè¾“å‡ºç¤ºä¾‹
        for (output_name, output_data) in &response.outputs {
            let sample_output = output_data.iter().take(3).collect::<Vec<_>>();
            println!("   ğŸ“¤ {}: {:?}", output_name, sample_output);
        }
        
        sleep(Duration::from_millis(30)).await;
    }
    
    Ok(())
}

/// å±•ç¤ºæ‰¹å¤„ç†æœåŠ¡
async fn demonstrate_batch_processing(engine: &mut AIEngine) -> Result<(), Error> {
    println!("\nğŸ“¦ å±•ç¤ºæ‰¹å¤„ç†æœåŠ¡...");
    
    // åˆ›å»ºæ‰¹å¤„ç†è¯·æ±‚
    let batch_sizes = vec![5, 10, 20];
    
    for batch_size in batch_sizes {
        println!("ğŸ”„ æ‰¹å¤„ç†å¤§å°: {} ä¸ªè¯·æ±‚", batch_size);
        
        let mut requests = Vec::new();
        
        for i in 0..batch_size {
            let mut inputs = HashMap::new();
            inputs.insert("input".to_string(), vec![i as f32 * 0.1, (i + 1) as f32 * 0.2]);
            
            let request = model_serving::InferenceRequest {
                model_name: "transformer_bert".to_string(),
                inputs,
                parameters: None,
                request_id: Some(format!("batch_req_{}_{}", batch_size, i)),
            };
            
            requests.push(request);
        }
        
        let batch_request = model_serving::BatchRequest {
            requests,
            batch_id: format!("batch_{}", batch_size),
            priority: 5,
        };
        
        let start = Instant::now();
        let response = engine.batch_inference(batch_request).await?;
        let duration = start.elapsed();
        
        println!("   âœ… æ‰¹å¤„ç†å®Œæˆ:");
        println!("      ğŸ“Š æˆåŠŸ: {} ä¸ª", response.success_count);
        println!("      âŒ å¤±è´¥: {} ä¸ª", response.error_count);
        println!("      â±ï¸  æ€»è€—æ—¶: {:.2}ms", duration.as_millis());
        println!("      ğŸš€ å¹³å‡è€—æ—¶: {:.2}ms/è¯·æ±‚", 
                duration.as_millis() as f64 / batch_size as f64);
        println!("      ğŸ“ˆ ååé‡: {:.0} è¯·æ±‚/ç§’", 
                batch_size as f64 / duration.as_secs_f64());
        
        sleep(Duration::from_millis(100)).await;
    }
    
    Ok(())
}

/// å±•ç¤ºè´Ÿè½½å‡è¡¡
async fn demonstrate_load_balancing(_engine: &mut AIEngine) -> Result<(), Error> {
    println!("\nâš–ï¸  å±•ç¤ºè´Ÿè½½å‡è¡¡...");
    
    // æ¨¡æ‹Ÿé«˜å¹¶å‘è¯·æ±‚
    let concurrent_requests = 50;
    let mut tasks = Vec::new();
    
    println!("ğŸš€ å¯åŠ¨ {} ä¸ªå¹¶å‘è¯·æ±‚", concurrent_requests);
    
    for i in 0..concurrent_requests {
        let task = tokio::spawn(async move {
            // æ¨¡æ‹Ÿæ¨ç†è¯·æ±‚
            let mut inputs = HashMap::new();
            inputs.insert("input".to_string(), vec![i as f32 * 0.01]);
            
            // æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            let start = Instant::now();
            tokio::time::sleep(Duration::from_millis(10 + (i % 20) as u64)).await;
            let duration = start.elapsed();
            
            (i, true, duration)
        });
        
        tasks.push(task);
    }
    
    // ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
    let mut success_count = 0;
    let mut total_time = Duration::from_millis(0);
    let mut max_time = Duration::from_millis(0);
    let mut min_time = Duration::from_millis(10000);
    
    for task in tasks {
        if let Ok((_i, success, duration)) = task.await {
            if success {
                success_count += 1;
            }
            total_time += duration;
            max_time = max_time.max(duration);
            min_time = min_time.min(duration);
        }
    }
    
    let avg_time = total_time / concurrent_requests as u32;
    
    println!("ğŸ“Š è´Ÿè½½å‡è¡¡ç»“æœ:");
    println!("   âœ… æˆåŠŸè¯·æ±‚: {}/{}", success_count, concurrent_requests);
    println!("   ğŸ“ˆ æˆåŠŸç‡: {:.1}%", (success_count as f64 / concurrent_requests as f64) * 100.0);
    println!("   â±ï¸  å¹³å‡å“åº”æ—¶é—´: {:.2}ms", avg_time.as_millis());
    println!("   ğŸš€ æœ€å¤§å“åº”æ—¶é—´: {:.2}ms", max_time.as_millis());
    println!("   âš¡ æœ€å°å“åº”æ—¶é—´: {:.2}ms", min_time.as_millis());
    println!("   ğŸ“Š ååé‡: {:.0} è¯·æ±‚/ç§’", 
            concurrent_requests as f64 / total_time.as_secs_f64());
    
    Ok(())
}

/// å±•ç¤ºæ€§èƒ½ç›‘æ§
async fn demonstrate_performance_monitoring(engine: &mut AIEngine) -> Result<(), Error> {
    println!("\nğŸ“Š å±•ç¤ºæ€§èƒ½ç›‘æ§...");
    
    // æ‰§è¡Œä¸€äº›æ“ä½œæ¥ç”Ÿæˆç›‘æ§æ•°æ®
    for i in 0..20 {
        let mut inputs = HashMap::new();
        inputs.insert("input".to_string(), vec![i as f32 * 0.1]);
        
        let request = model_serving::InferenceRequest {
            model_name: "transformer_bert".to_string(),
            inputs,
            parameters: None,
            request_id: Some(format!("monitor_req_{}", i)),
        };
        
        let _ = engine.inference(request).await;
        
        // æ¯5ä¸ªè¯·æ±‚æ˜¾ç¤ºä¸€æ¬¡ç›‘æ§æ•°æ®
        if i % 5 == 0 {
            let stats = engine.get_model_service_stats().await;
            
            println!("ğŸ“ˆ ç›‘æ§æ•°æ® (è¯·æ±‚ {}):", i + 1);
            println!("   ğŸ”¢ æ€»è¯·æ±‚æ•°: {}", stats.get("total_requests").unwrap_or(&"0".to_string()));
            println!("   âŒ é”™è¯¯æ•°: {}", stats.get("total_errors").unwrap_or(&"0".to_string()));
            println!("   ğŸ“Š é”™è¯¯ç‡: {}", stats.get("error_rate").unwrap_or(&"0.00%".to_string()));
            println!("   ğŸš€ æœåŠ¡ä¸­æ¨¡å‹: {}", stats.get("serving_models").unwrap_or(&"0".to_string()));
            
            if let Some(processing_time) = stats.get("metric_inference_processing_time_ms") {
                println!("   â±ï¸  å¹³å‡å¤„ç†æ—¶é—´: {}ms", processing_time);
            }
        }
        
        sleep(Duration::from_millis(20)).await;
    }
    
    Ok(())
}

/// å±•ç¤ºæ¨¡å‹ç‰ˆæœ¬ç®¡ç†
async fn demonstrate_model_versioning(engine: &mut AIEngine) -> Result<(), Error> {
    println!("\nğŸ”„ å±•ç¤ºæ¨¡å‹ç‰ˆæœ¬ç®¡ç†...");
    
    let model_name = "transformer_bert";
    
    // åŠ è½½v1.0ç‰ˆæœ¬
    println!("ğŸ“¦ åŠ è½½æ¨¡å‹ç‰ˆæœ¬ v1.0");
    let model_v1 = create_model_config(model_name, "v1.0", ModelType::NLP);
    engine.load_model_to_service(model_v1).await?;
    engine.start_model_serving(model_name).await?;
    
    // æ‰§è¡Œä¸€äº›æ¨ç†
    for i in 0..3 {
        let mut inputs = HashMap::new();
        inputs.insert("input".to_string(), vec![i as f32]);
        
        let request = model_serving::InferenceRequest {
            model_name: model_name.to_string(),
            inputs,
            parameters: None,
            request_id: Some(format!("v1_req_{}", i)),
        };
        
        let _ = engine.inference(request).await;
        println!("   âœ… v1.0 æ¨ç†è¯·æ±‚ {} å®Œæˆ", i + 1);
    }
    
    // åœæ­¢v1.0æœåŠ¡
    engine.stop_model_serving(model_name).await?;
    engine.unload_model_from_service(model_name).await?;
    
    // ç­‰å¾…ä¸€ä¸‹ç¡®ä¿æ¨¡å‹å®Œå…¨å¸è½½
    sleep(Duration::from_millis(100)).await;
    
    // åŠ è½½v2.0ç‰ˆæœ¬
    println!("ğŸ“¦ å‡çº§åˆ°æ¨¡å‹ç‰ˆæœ¬ v2.0");
    let model_v2 = create_model_config(model_name, "v2.0", ModelType::NLP);
    engine.load_model_to_service(model_v2).await?;
    engine.start_model_serving(model_name).await?;
    
    // æ‰§è¡Œä¸€äº›æ¨ç†
    for i in 0..3 {
        let mut inputs = HashMap::new();
        inputs.insert("input".to_string(), vec![i as f32 * 2.0]);
        
        let request = model_serving::InferenceRequest {
            model_name: model_name.to_string(),
            inputs,
            parameters: None,
            request_id: Some(format!("v2_req_{}", i)),
        };
        
        let _ = engine.inference(request).await;
        println!("   âœ… v2.0 æ¨ç†è¯·æ±‚ {} å®Œæˆ", i + 1);
    }
    
    println!("ğŸ‰ æ¨¡å‹ç‰ˆæœ¬ç®¡ç†æ¼”ç¤ºå®Œæˆ");
    
    Ok(())
}

// è¾…åŠ©å‡½æ•°

/// åˆ›å»ºæ¨¡å‹é…ç½®
fn create_model_config(name: &str, version: &str, model_type: ModelType) -> ModelConfig {
    let mut parameters = HashMap::new();
    parameters.insert("batch_size".to_string(), Value::Number(serde_json::Number::from(32)));
    parameters.insert("learning_rate".to_string(), Value::Number(serde_json::Number::from_f64(0.001).unwrap()));
    
    ModelConfig {
        name: name.to_string(),
        version: version.to_string(),
        model_type,
        framework: Some("candle".to_string()),
        parameters,
        path: Some(format!("/models/{}/{}", name, version)),
        device: Some("cuda".to_string()),
        precision: Some("fp16".to_string()),
    }
}

/// æ‰“å°æ¨¡å‹æœåŠ¡ç»Ÿè®¡ä¿¡æ¯
async fn print_model_service_stats(engine: &AIEngine) {
    let stats = engine.get_model_service_stats().await;
    let gpu_stats = engine.get_gpu_performance_stats();
    
    println!("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("â”‚                   æ¨¡å‹æœåŠ¡ç»Ÿè®¡                        â”‚");
    println!("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
    println!("â”‚ æ€»æ¨¡å‹æ•°: {:<42} â”‚", stats.get("total_models").unwrap_or(&"0".to_string()));
    println!("â”‚ æœåŠ¡ä¸­æ¨¡å‹: {:<40} â”‚", stats.get("serving_models").unwrap_or(&"0".to_string()));
    println!("â”‚ æ€»è¯·æ±‚æ•°: {:<42} â”‚", stats.get("total_requests").unwrap_or(&"0".to_string()));
    println!("â”‚ é”™è¯¯ç‡: {:<44} â”‚", stats.get("error_rate").unwrap_or(&"0.00%".to_string()));
    println!("â”‚ æœ€å¤§å¹¶å‘: {:<42} â”‚", stats.get("max_concurrent_requests").unwrap_or(&"0".to_string()));
    println!("â”‚ æ‰¹å¤„ç†å¤§å°: {:<40} â”‚", stats.get("batch_size").unwrap_or(&"0".to_string()));
    println!("â”‚ GPUè®¾å¤‡: {:<44} â”‚", gpu_stats.get("device_name").unwrap_or(&"æœªçŸ¥".to_string()));
    println!("â”‚ æ˜¾å­˜ä½¿ç”¨: {:<44} â”‚", gpu_stats.get("memory_free_gb").unwrap_or(&"0".to_string()));
    println!("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
    
    println!("\nğŸš€ æ€§èƒ½æŒ‡æ ‡:");
    if let Some(processing_time) = stats.get("metric_inference_processing_time_ms") {
        println!("   â€¢ å¹³å‡æ¨ç†æ—¶é—´: {}ms", processing_time);
    }
    if let Some(batch_time) = stats.get("metric_batch_processing_time_ms") {
        println!("   â€¢ æ‰¹å¤„ç†æ—¶é—´: {}ms", batch_time);
    }
    if let Some(success_rate) = stats.get("metric_batch_success_rate") {
        println!("   â€¢ æ‰¹å¤„ç†æˆåŠŸç‡: {:.1}%", success_rate.parse::<f64>().unwrap_or(0.0) * 100.0);
    }
}

// æ³¨æ„ï¼šåœ¨å®é™…ä½¿ç”¨ä¸­ï¼ŒAIEngineå¯èƒ½éœ€è¦æ›´å¤æ‚çš„å¹¶å‘å¤„ç†
// è¿™é‡Œä½¿ç”¨Arc<Mutex<AIEngine>>æˆ–ç±»ä¼¼çš„æ–¹å¼æ¥å¤„ç†å¹¶å‘è®¿é—®
